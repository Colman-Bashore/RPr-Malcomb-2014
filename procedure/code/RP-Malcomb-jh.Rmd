---
output: html_document 
---

# Reproduction of Malcomb et al 2014

#### Malcomb, D. W., E. A. Weaver, and A. R. Krakowka. 2014. Vulnerability modeling for sub-Saharan Africa: An operationalized approach in Malawi. Applied Geography 48:17-30.

#### [https://doi.org/10.1016/j.apgeog.2014.01.004]([https://doi.org/10.1016/j.apgeog.2014.01.004)

### Authors: Kufre Udoh, Drew An-Pham, Joseph Holler, and Middlebury College Spring 2019 + 2021 Geography 323 Class

### [https://gis4dev.github.io/](https://gis4dev.github.io/)


```{r libraries, include = F}

packages = c("downloader","haven","stars","dplyr","sf","rdhs", "classInt", "readr", "ggplot2", "here", "s2")
setdiff(packages, rownames(installed.packages()))
install.packages(setdiff(packages, rownames(installed.packages())), quietly=TRUE)

library(downloader)
library(haven)
library(sf)
library(stars)
library(dplyr)
library(here)
library(classInt)
library(rdhs)
library(readr)
library(ggplot2)
library(s2)
library(pastecs)

sf_use_s2(T)
```

```{r formatting}
# list of required packages
packages = c("here")

# load and install required packages
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# save the R processing environment to r_environment.txt
writeLines(capture.output(sessionInfo()),here("procedure","environment","r_environment.txt"))
```

```{r download data}

private_r = here("data","raw","private")
public_r = here("data","raw","public")

if (!"traditional_authorities" %in% list.files(public_r)){
  # Malawi administrative areas from GADM version 2.8 https://gadm.org/download_country_v2.html
  download("https://biogeo.ucdavis.edu/data/gadm2.8/shp/MWI_adm_shp.zip", here("data","scratch", "MWI_adm_shp.zip"))
  unzip(here("data","scratch", "MWI_adm_shp.zip"), exdir = here("data","raw","public"))
}

if (!"livelihood_zones" %in% list.files(public_r)){
  # Malawi livelihood zones from FEWS NET Data Center https://fews.net/fews-data/335
  download("https://fews.net/data_portal_download/download?data_file_path=http%3A//shapefiles.fews.net.s3.amazonaws.com/LHZ/MW_LHZ_2009.zip", here("data","scratch","MW_LHZ_2009.zip"))
  unzip(here("data","scratch","MW_LHZ_2009.zip"), exdir = here("data","raw","public"))
}

if (!"major_lakes.csv" %in% list.files(public_r)) {
  # major lakes in malawi: http://www.masdap.mw/
  download(
    "http://www.masdap.mw/geoserver/ows?outputFormat=csv&service=WFS&srs=EPSG%3A4326&request=GetFeature&typename=geonode%3Amajor_lakes&version=1.0.0",
    here("data","raw","public","major_lakes.csv")
  )
}
```

```{r dhs data access configuration}
email = readline(prompt="Enter DHS Login Email: ")
project = readline(prompt="Enter Project Name: ")
rdhs_json = here("data","raw","private","rdhs.json")

if (!file.exists(rdhs_json)) file.create(rdhs_json)

# the information here was established through DHS project approval. See dhs-metadata.md in the data/metadata folder for details.
# running this function will prompt you to enter email and project information in the Console and password in a popup
set_rdhs_config(
  email = email,
  project = project,
  config_path = rdhs_json,
  global = FALSE,
  cache_path = here("data","raw","private")
)

```

```{r downloading dhs data}
dhs_downloads = get_datasets(
  c("MWHR61SV", "MWGE62FL"), #removed latter two datasets, as they were for 2004; the two that are used here are for 2010
  all_lower = FALSE,
  download_option = "rds"
)
# common error: Error in names(filedatatypelist_DHS) <- paste0("filedatatypelist_", qdapRegex::rm_between(filedatatypelist_DHS_line, : 'names' attribute [1] must be the same length as the vector [0]

saveRDS(dhs_downloads, here("data","raw","private","dhs_downloads.rds"))
#read RDS at the beginning of the next section

readRDS(dhs_downloads, here("data","raw","private","dhs_downloads.rds"))
```

```{r REMAINING DATA IMPORTS -- need to figure out how to arange this + the former}
# read raster data into r
# UNEP layers
# set CRS for raster
dr = read_stars(here("data", "raw", "public", "dr1010ipeykx.tif")) %>% 
  st_set_crs(4326) 

fl = read_stars(here("data", "raw", "public",  "fl1010irmt.tif")) %>% 
  st_set_crs(4326) 

lhz_v = read_sf(here("data", "raw", "public", "MW_LHZ_2009.shp")) %>% st_make_valid()

lhz_t = read.csv(here(private_r, "lhz.csv"))

ta = read_sf(here("data", "raw", "public", "MWI_adm2.shp")) %>%
  st_make_valid() 

ta_hh_gref = read_sf(here("data", "derived", "public", "ta_v.shp")) %>% st_make_valid()

lakes = st_as_sf(read_csv(here(public_r, "major_lakes.csv"))[, c("name", "the_geom")],
                 wkt = "the_geom",
                 crs = 4326) %>%
  st_geometry %>%
  st_union %>%
  st_sf %>%
  mutate(EA = "Major Lakes of Malawi")
```

```{r filter tas & fix geometries}
ta_nature <- subset(ta, ENGTYPE_2 == 'Water body' | ENGTYPE_2 == 'National Park' | ENGTYPE_2 == 'Reserve') %>%
  filter(ID_2 != 162) %>% #combine arguments here
  filter(ID_2 != 224)
#25 features (this removes the errors) 
ta_populated <- subset(ta, ENGTYPE_2 == 'City' | ENGTYPE_2 == 'Headquarter' | ENGTYPE_2 == 'Sub-chief' | ENGTYPE_2 == 'Town' | ENGTYPE_2 == 'Traditional Authority'| ENGTYPE_2 == 'Urban')
#229 features
ta_errors <- ta[c(162, 224), ]

#creates a buffer around the lakes & uses this as a mask to clip two 'outlier' features from Traditional Authorities
# alternative: library(spatialEco) --> explode(ta_no_case, sp=FALSE) https://www.rdocumentation.org/packages/spatialEco/versions/1.3-1/topics/explode
ta_v = st_difference(st_transform(filter(ta_errors), 3395), st_buffer(st_transform(lakes, 3395), 625)) %>%
  st_transform(4326) %>%
  st_cast ("POLYGON", warn=FALSE)

# select out 5 largest polygon features (the ones we actually care about)
ta_v <- ta_v[c(1:4,17),]
ta_v <- ta_v %>%
  mutate(ID_2 = ID_2 + row_number() * 1000) %>% #gives unique IDs different values
  select(-c(EA)) %>%
  st_cast ("MULTIPOLYGON", warn=FALSE)

ta_v <- bind_rows(ta_populated, ta_v)

st_write(ta_v, here("data", "derived", "public", "ta_v.shp"), append=FALSE) 

ta_v_check <- ggplot() + geom_sf(data = ta_v)
ta_v_check 
```

```{r 2010 adaptive capacity data}
dhsclusters_2010 = readRDS(here("data", "raw", "private", "datasets", "MWGE62FL.rds")) %>%
  as("sf") %>% 
  st_transform(3395) %>% 
  # joining id for traditional authorities and livelihood zones to dhs clusters
  st_join(st_transform(select(ta_hh_gref, ID_2),3395)) %>% #FOR revision: you want to join to good TAs, refer to workflow 7/6 & count dhs clusters + households #survey clusters to TAs #keep such id unique
  st_join(st_transform(select(lhz_v, FNID),3395)) %>% 
  rename(ta_id = ID_2,
         lhz_id = FNID,
         urban_rural = URBAN_RURA)

dhshh_2010 = readRDS(here("data", "raw", "private", "datasets", "MWHR61SV.rds")) %>% zap_labels()
```

```{r households to remove (2010)}
rmv_2010 = dhshh_2010 %>%  filter(
  HV246A == 98 |
    HV246A == 99 |
    HV246D == 98 |
    HV246D == 99 |
    HV246E == 98 |
    HV246E == 99 |
    HV246G == 98 |
    HV246G == 99 |
    HV219  == 9 |
    HV243A == 9 |
    HV245  == 99 |
    HV206  == 9 |
    HV204  == 999 |
    HV204  == 998 |
    HV226  == 99 |
    HV226  == 95 |
    HV226  == 96 |
    HV207  ==  9 
) %>% pull(HHID)
```

```{r capacity in households 2010}
hh_capacity_t = dhshh_2010 %>%
  # joining traditional authority ids and urban_rural column 
  left_join(st_drop_geometry(select(dhsclusters_2010, DHSCLUST, ta_id, urban_rural)), by = c("HV001" = "DHSCLUST")) %>%
  select(
    HHID,
    HV001,
    HV002,
    ta_id,
    urban_rural,
    HV246A,
    HV246D,
    HV246E,
    HV246G,
    HV248,
    HV245,
    HV271,
    HV251,
    HV204,
    HV206,
    HV226,
    HV219,
    HV243A,
    HV207
    #25231 features
  ) %>%
  # removing values based on index and where there are NAs 
  filter(!HHID %in% rmv_2010) %>% 
  filter(!is.na(ta_id)) %>% 
  # 24430 obs. of 19 variables 
  # removing any surveys where all livestock values were NA
  filter(!(is.na(HV246A) & is.na(HV246D) & is.na(HV246E)  & is.na(HV246G) )) %>% 
  # 24428 obs. of 19 variables 
  # using rowwise() to find sum of livestock by household --> go through every row of table
  # and calulate something for each one
  rowwise %>%
  mutate(hhlivestock = sum(HV246A, HV246D, HV246E, HV246G, na.rm = T)) %>%
  ungroup %>%
  # using percent_rank(), those  
  # in cases where desc() is used, having a greater value before ranked makes a household more vulnerable 
  mutate(
    livestock = percent_rank(hhlivestock) * 4,
    sick = percent_rank(desc(HV248)) * 4,
    land = percent_rank(HV245) * 4,
    wealth = percent_rank(HV271) * 4,
    orphans = percent_rank(desc(HV251)) * 4,
    # changing 996 to 0 as it takes no time to get water on premises
    # percenrank sets values 0 to 1, then * 4 + 1 makes the range 1-5
    # femalehh male = 1, female = 2, run 0 to 2.81 is the binary
    HV204 = ifelse(HV204 == 996, 0, HV204), #996 means they have source of water in proximity to home
    water = percent_rank(desc(HV204)) * 4,
    electricity = percent_rank(HV206) * 4,
    cooking = percent_rank(desc(HV226)) * 4,
    femalehh = percent_rank(desc(HV219)) * 4,
    cellphone = percent_rank(desc(HV243A)) * 4,
    radio = percent_rank(HV207) * 4,
    urbanruralscore = ifelse(urban_rural == "U", 4, 0)
    # everything is now on a 0-4 scale
  ) 
```

```{r capacity for traditional authorities 2010 at the household level}
hh_capacity_t <- hh_capacity_t %>%
  # calculating capacity score based on table 2 in malcomb et al 
  rowwise %>%
  mutate(
    capacity = sum(
      livestock * 0.04,
      sick * 0.03,
      land * 0.06,
      wealth * 0.04,
      orphans * 0.03,
      water * 0.04,
      electricity * 0.03,
      cooking * 0.02,
      femalehh * 0.02,
      cellphone * 0.04,
      radio * 0.03,
      urbanruralscore * 0.02),
      # NAs are not removed here to filter out incomplete surveys later on
      # multiple by ratio that adds everything to 40% of 5 --> max is going to be 2, 2*20 = 40%
      # at this point, values have possible range of 0 to 2 -- but capacity has actual range of 0.471 to 1.454 for 1-5 rnage
      # values have possible range of 0 to 1.6, 0.071 to 1.054 when adjusted range of 0-4
      na.rm = F)
```

``` {r summary stats for tas at hh level before pullings nas}
hh_stats <- stat.desc(hh_capacity_t[,21:33]) %>%
  mutate_if(is.numeric, round, digits=1)
hh_stats
```

```{r extract traditional authorities without data}
hh_null <- hh_capacity_t[is.na(hh_capacity_t$capacity),] 

st_write(hh_null, here("data", "derived", "public", "hh_null.csv"), append=FALSE)
# run the function up to na.rm = F, then run this function
#4094 features
```

```{r remove nulls}
hh_capacity_t <- hh_capacity_t %>%
filter(!is.na(capacity)) %>% # removing incomplete surveys 
ungroup
#19562 features (from the initial 23543 --> removed 3981 NA features)
```

``` {r summary stats for tas at hh level after pullings nas}
hh_capacity_stats <- stat.desc(hh_capacity_t[,21:33]) %>%
  mutate_if(is.numeric, round, digits=1)
hh_capacity_stats
```

```{r capacity for traditional authorities 2010; aggregates from household level}
  ta_capacity_t <- hh_capacity_t %>% 
  group_by(ta_id) %>%
  summarize(
    capacity_avg = mean(capacity),
    capacity_min = min(capacity),
    capacity_max = max(capacity),
    capacity_sd = sd(capacity)
  )
#214 features
```

```{r joining 2010 capacity to ta and creating breaks for visualization}
# join mean capacity to traditional authorities
ta_v = left_join(
  ta_v,
  select(ta_capacity_t, ta_id, capacity_2010 = capacity_avg),
  by = c("ID_2" = "ta_id")
)

# making capacity score resemble malcomb et al's work 
ta_v = mutate(ta_v, capacity_2010 = capacity_2010 * 25) # --> THIS LINE IS NOW REMOVED
# 234 features 

# preparing breaks for mapping using natural jenks method
# USE MANUAL BREAKS HERE
ta_brks = filter(ta_v, !is.na(capacity_2010)) %>% {classIntervals(.$capacity_2010, 4, style = "jenks")$brks}

ta_int = lapply(1:4, function(x) paste0(round(ta_brks[x],2)," - ", round(ta_brks[x +1],2))) %>% unlist()

ta_v = mutate(ta_v, capacity_2010_brks = case_when(
  capacity_2010 <= ta_brks[2] ~ ta_int[1],
  capacity_2010 <= ta_brks[3] ~ ta_int[2],
  capacity_2010 <= ta_brks[4] ~ ta_int[3],
  capacity_2010 >  ta_brks[4] ~ ta_int[4]
))
```

```{r extract traditional authorities without data by capacity}
ta_null_cap <- ta_v[is.na(ta_v$capacity_2010),]
#20 features

st_write(ta_null_cap, here("data", "derived", "public", "ta_null_capacity.shp"), append=FALSE)
```

```{r saving adaptive capacity scores}
save(
  ta_capacity_t,
  file = here("data", "derived", "public", "adaptive_capacity.rData")
)
```

```{r check final ta_v output before mapping}
ta_v_check <- ggplot() + 
  geom_sf(data = ta_v) +
  geom_sf(data = ea,
          aes(fill = EA),
          color = NA) +
    geom_sf(
    data = ta_v,
    aes(fill = capacity_2010_brks),
    color = "white",
    lwd = .2
  ) + scale_fill_manual(
    #  unique(ta_2010$capacity_2010_brks) allows you to check breaks
    values = list(
      "Missing Data" = "#FFC389",
      "National Parks and Reserves" = "#D9EABB",
      "Major Lakes of Malawi" = "lightblue",
      "7.43 - 9.82" = "#333333",
      "9.82 - 11.39" = "#666666",
      "11.39 - 13.6" = "#999999",
      "13.6 - 16.68" = "#CCCCCC"
    ))
ta_v_check 
```

```{r reading rasters into r}
# UNEP layers
# set CRS for raster
dr = read_stars(here("data", "raw", "public", "dr1010ipeykx.tif")) %>% 
  st_set_crs(4326) 

fl = read_stars(here("data", "raw", "public",  "fl1010irmt.tif")) %>% 
  st_set_crs(4326) 
```

```{r cleaning and reprojecting rasters}
# creating blank raster in extent
# extent of the raster layers
# units used for bounding box = decimal degrees, long-lat
b = st_bbox(
  c(
    xmin = 35.9166666666658188,
    xmax = 32.6666666666658330,
    ymin = -9.3333333333336554,
    ymax = -17.0833333333336270
  ),
  crs = st_crs(4326)
) %>%
  st_as_sfc()

# where raster = 1, you have flood visualized
blank = st_as_stars(st_bbox(b), dx = 0.041667, dy = 0.041667)
blank[[1]][] = NA

# reprojecting, clipping, and resampling rasters to new extent and cell size
# use bilinear for drought to average continuous population exposure values (resampling?)
# (object, destination, logical operator, usable b/c of GDAL)
dr = st_warp(dr, blank, use_gdal = T, method = "bilinear")
# use nearest neighbor for flood risk to preserve integer values
fl = st_warp(fl, blank, method = "near")  

# removing factors from fl
# You can use double brackets to select elements in more or less the same way as single brackets. The difference between single and double is that with double brackets any element names are not displayed:
# bracket list, find rows with a double bracket??? ([1] is the first band, brackets filter down)
# rasters are a raised matrix (multiple layer) access first level of the array, second bracket editing values into the array
nmrc = as.numeric(levels(fl[[1]]))[fl[[1]]]
fl = blank
# single bracket gives you meta data (first layer), double brackets gives it to you as a matrix
fl[[1]][] = nmrc
```

```{r rasterizing geometries}
# clipping traditional authorities with livelihood zones in order to remove lake and existing null values
# building a clip function in R [x,y are parameters]
st_clip = function(x,y) st_intersection(x, st_union(st_geometry(y)))

st_agr(ta_v) = "constant"

#creates a small buffer around the Livelihood Zones & use this as a mask to clip features from Traditional Authorities (only the TAs that have data clipped to the extent of Malawi)
ta_2010 = st_clip(st_transform(filter(ta_v, is.na(capacity_2010) == F), 3395), st_buffer(st_transform(lhz_v, 3395), .01)) %>%
  st_transform(4326)
# 214 features 

# making capacity rasters 
ta_capacity_r = st_rasterize(ta_2010[, 'capacity_2010'], blank)
```

```{R processing + weighting FEWSNet Data}

# LHZ Data

lhz_v = read_sf(here("data", "raw", "public", "MW_LHZ_2009.shp")) %>% st_make_valid()

lhz_t = read.csv(here(private_r, "lhz.csv"))

lhz_sensitivity_v  = lhz_v %>%
  inner_join(lhz_t, by = ("LZCODE" = "LZCODE")) %>%
  
  
  mutate(
  pctOwnCrop = (sof_crop + sof_livestock) * 100, 
  pctIncWage = (soc_labour / soc_total) * 100,
  pctIncCashCrops = (cp_tobacco + cp_sugar + cp_tea + cp_coffee) / soc_total * 100,
  disasterCoping = (se_firewoord + se_grass	+ se_wildfood + se_charcoal + se_matmaking + se_basket) / soc_total * 100,
  ) %>%

  mutate(
  ownCrop = percent_rank(pctOwnCrop) * 4, # high values means lower sensitivity 
  wageIncome = percent_rank(pctIncWage) * 4, # high values means lower sensitivity
  cashCropIncome = percent_rank(desc(pctIncCashCrops)) * 4, #high values mean lower sensitivity (b/c of desc)
  newDisaster = percent_rank(desc(disasterCoping)) * 4, #high values mean lower sensitivity (b/c of desc)
  ) %>%
  rowwise %>%
  mutate(
    livelihoodSensitivity_new = sum(
      ownCrop * 6,
      wageIncome * 6,
      cashCropIncome * 4,
      newDisaster * 4) 
  ) %>% ungroup
 # %>% ungroup
# the theoretical range is 0 to 0.8 [rescale & multiply by 25 20/.8]
# livelihood sensitivity range is now .235 to .490
# making capacity rasters 
lhz_sensitivity_r = st_rasterize(lhz_sensitivity_v[,'livelihoodSensitivity_new'], blank)

lhz_v_check <- ggplot() + geom_sf(data = lhz_sensitivity_v) #make this choropleth
lhz_v_check
```

```{r summary table with lhz data}
lhz_stats <- st_drop_geometry(lhz_sensitivity_v[,14:36]) %>%
  stat.desc() %>%
  mutate_if(is.numeric, round, digits=1)
lhz_stats
```

```{r function to calculate vulnerability}
  # creating mask layer
  mask = ta_capacity 
  mask[mask > 0] = 1
  mask[mask == 0] = NA
  
  # masking flood and drought 
  #reclassify drought & flood already has classification
  #we want flood to have 20% weight so we multiply by 5, as is the data has a range of 0-4
  flood = fl * mask * 5
  drought = dr * mask
  
  # makes a list of quintile break points
  qt = quantile(drought[[1]], probs = seq(0, 1, 0.2), na.rm = T)
  
  # reclassifying drought layer using break points from 0 to 4
  # 4 * 5 = 20, match 20% weighting of exposition to drought events
  drought = drought %>%
    mutate(
      recoded = case_when(
        drought[[1]] <= qt[[2]] ~ 0,
        drought[[1]] <= qt[[3]] ~ 1,
        drought[[1]] <= qt[[4]] ~ 2,
        drought[[1]] <= qt[[5]] ~ 3,
        drought[[1]] > qt[[5]] ~ 4
      )
    ) %>% select(recoded) * 5
  
  # final output (adding component rasters)
  final = 100 - (ta_capacity_r + lhz_sensitivity_r + (40 - (drought + flood))) # 100 = highest vulnerability, 0 = lowest vulnerability
```

```{r descriptive stats for raster inputs}
fl_stats <- stat.desc(fl)
dr_stats <- stat.desc(dr)
fl
dr
#do you need standard deviation here?
```

```{r creating final vulnerability layers}
ta_2010$vuln = aggregate(final,ta_2010,mean)$capacity_2010 #THIS IS THE REAL Q, what's the mean doing

#rasterize 
ta_r = ta_2010[!is.na(ta_2010$vuln), 18]
ta_r = st_rasterize(ta_r[1])
```

```{r extract traditional authorities without data by vuln}
ta_null <- ta_2010[is.na(ta_2010$vuln),]
#20 features

st_write(ta_null, here("data", "derived", "public", "ta_null.shp"), append=FALSE)
```

```{r misc. map features}
lakes = st_as_sf(read_csv(here(public_r, "major_lakes.csv"))[, c("name", "the_geom")],
                 wkt = "the_geom",
                 crs = 4326) %>%
  st_geometry %>%
  st_union %>%
  st_sf %>%
  mutate(EA = "Major Lakes of Malawi")

#ea used to retrieve national parks (conservation areas) + lakes
ea = lhz_v %>%
  st_transform(3395) %>%  #transform to world mercator (jh: not sure if we need to transform to 3395 and back here?)
  summarize %>%  
  st_geometry %>%  #dissolve to one feature / one geometry
  st_intersection(st_geometry(st_transform(ta, 3395))) %>%   #intersect with traditional authorities to clip them
  st_transform(4326) %>%
  st_sf %>%   #make into new simple features data frame
  mutate(EA = case_when(
    grepl("Reserve", ta[["NAME_2"]]) | grepl("Park", ta[["NAME_2"]]) ~ "National Parks and Reserves",
    T ~ "Missing Data")   # search and replace names- anything with Reserve or Park in the name becomes National Parks and Reserves
  ) %>%
  rbind(lakes) %>%
  st_make_valid()
```

```{r 2010 adaptive capacity map}
map_2010 = ggplot() +
  geom_sf(data = ea,
          aes(fill = EA),
          color = NA) +
  geom_sf(
    data = ta_2010,
    aes(fill = capacity_2010_brks),
    color = "white",
    lwd = .2
  ) + scale_fill_manual(
    #  unique(ta_2010$capacity_2010_brks) allows you to check breaks
    values = list(
      "Missing Data" = "#FFC389",
      "National Parks and Reserves" = "#D9EABB",
      "Major Lakes of Malawi" = "lightblue",
      "7.43 - 9.82" = "#333333",
      "9.82 - 11.39" = "#666666",
      "11.39 - 13.6" = "#999999",
      "13.6 - 16.68" = "#CCCCCC"
    )
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Adaptive Capacity Scores Based on 2010 DHS Surveys in 222 Traditional Authorities") +
  theme_minimal() +
  theme(legend.title = element_blank())

map_2010
```

```{r find break & limit values for vulnerability map}
vuln_max <- max(ta_r[[1]],na.rm = TRUE)
vuln_min <- min(ta_r[[1]],na.rm = TRUE)
``` 

```{r vulnerability map}
clrs = mutate(
  ea,
  colors = case_when(
    EA == "Missing Data" ~ "#999999",
    EA == "National Parks and Reserves" ~ "#D9EABB",
    EA == "Major Lakes of Malawi" ~ "lightblue"
  )
)$colors

vuln_map = ggplot() +
  geom_sf(data = ea,
          fill = clrs,
          color = NA) +
  geom_stars(data = ta_r) +
  scale_fill_gradient(
    low = "#FFFF75",
    high = "#CF4611",
    breaks = c(vuln_min, vuln_max),
    labels = c("Lower Vulnerability", "Higher Vulnerability"),
    na.value = "transparent",
    guide = "colourbar",
    limits = c(vuln_min, vuln_max)
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Malawi Vulnerability to Climate Change") +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )

vuln_map
```

```{r saving fig 4 and 5 maps}

ggsave(
  here("results","figures","fig4rep_final.png"),
  plot = map_2010,
  width = 8.5,
  height = 11,
  units = "in"
)
ggsave(
  here("results","figures","fig5rep_final.png"),
  plot = vuln_map,
  width = 8.5,
  height = 11,
  units = "in"
)
```

```{r saving spatial data outputs}
results = here("data","derived","public","results.gpkg")

write_stars(ta_r, here("data","derived","public","ta_capacity.tif"))

write_sf(ta_2010, results, "ta_2010")

write_sf(lhz_v, results, "lhz")
```

```{R Sample Code for Comparing Discrete Choropleth Maps}
or_fig4 = # load original figure 4 data
  read_sf(here("data", "derived", "public", "ta_v_comp.shp"), 
          layer="ta_v_comp") %>% 
  # load ta_resilience layer from georeferencing geopackage
  st_drop_geometry() %>%
  # remove the geometry data because two geometries cannot be joined
  select(c(ID_2,resilience)) %>%  
  # select only the ID_2 and resilience columns
  na.omit()
  # remove records with null values

rp_fig4 = ta_2010 %>% # prepare our reproduction of figure 4 data
  select(c(ID_2,capacity_2010)) %>%  
  # select only the ID_2 and resilience columns
  # note: geometry columns are 'sticky' -- only way to remove is st_drop_geometry()
  na.omit()  %>%
  # remove records with null values
  mutate(rp_res = case_when(
  capacity_2010 <= ta_brks[2] ~ 1,
  capacity_2010 <= ta_brks[3] ~ 2,
  capacity_2010 <= ta_brks[4] ~ 3,
  capacity_2010 >  ta_brks[4] ~ 4
))
# code the capacity scores as integers, as we see them classified on the map. 
#ta_brks was the result of a Jenks classification, as noted on Malcomb et al's maps

fig4compare = inner_join(rp_fig4,or_fig4,by="ID_2") %>%  
  #inner join on field ID_2 keeps only matching records
  filter(rp_res>0 & rp_res<5 & resilience > 0 & resilience < 5)
  # keep only records with valid resilience scores

table(fig4compare$resilience,fig4compare$rp_res)
# crosstabulation with frequencies

cor.test(fig4compare$resilience,fig4compare$rp_res,method="spearman")
# Spearman's Rho correlation test

fig4compare = mutate(fig4compare, difference = rp_res - resilience) 
# Calculate difference between the maps so that you can create a difference map
```

```{R Sample Code for Comparing Continuous Raster Maps}
orfig5vect = 
  read_sf(here("data", "derived", "public", "georef_bg.gpkg"), 
          layer="georef_bg")
# load original figure 5 data

orfig5rast = st_rasterize(orfig5vect["bg_mean"], template=ta_r)
# convert mean of blue values into a raster using ta_final as a reference for raster
# extent, cell size, CRS, etc.
# blue + green [LOOK BACK AT ZONAL STATS]

orfig5rast = orfig5rast %>% 
  mutate(or = 1-
           (bg_mean - min(orfig5rast[[1]], na.rm= TRUE)) /
           (max(orfig5rast[[1]], na.rm= TRUE) -
            min(orfig5rast[[1]], na.rm= TRUE)
        )
)  # or is Re-scaled from 0 to 1 with (value - min)/(max - min)
# it is also inverted, because higher blue values are less red

ta_r = ta_r %>% 
  mutate(rp =
           (vuln - min(ta_r[[1]], na.rm= TRUE)) /
           (max(ta_r[[1]], na.rm= TRUE) -
            min(ta_r[[1]], na.rm= TRUE)
        )
)  # rp is Re-scaled from 0 to 1 with (value - min)/(max - min)

fig5comp = c( select(ta_r,"rp"), select(orfig5rast,"or"))
# combine the original (or) fig 5 and reproduced (rp) fig 5

fig5comp = fig5comp %>% mutate( diff = rp - or )
# calculate difference between the original and reproduction,
# for purposes of mapping

fig5comppts = st_as_sf(fig5comp)
# convert raster to vector points to simplify plotting and correlation testing

plot(fig5comppts$or, fig5comppts$rp, xlab="Original Study", ylab="Reproduction")
title("Comparing Vulnerability Scores")
# create scatterplot of original results and reproduction results

cor.test(fig5comppts$or, fig5comppts$rp, method="spearman")
# Spearman's Rho correlation test

# Hint for mapping raster results: refer to the diff raster attribute
# in the fig5comp stars object like this: fig5comp["diff"]
```

```{r 2010 adaptive capacity difference map}
ac_diff_map = ggplot() +
  geom_sf(data = ea,
          aes(fill = EA),
          color = NA) +
  geom_sf(
    data = fig4compare,
    aes(fill = factor(difference)),
    color = "white",
    lwd = .2
  ) + scale_fill_manual(limits = c("-2","-1","0","1","Missing Data","Major Lakes of Malawi","National Parks and Reserves"),
    values = list(
      "Missing Data" = "#FFFFFF",
      "National Parks and Reserves" = "#D9EABB",
      "Major Lakes of Malawi" = "lightblue",
      "-2" = "#e66101",
      "-1" = "#fdb863",
      "0" = "#cccccc",
      "1" = "#b2abd2"
    )
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Fig. 4 Replication Comparison") +
  theme_minimal() +
  theme(legend.title = element_blank())

ac_diff_map
```

```{r find break & limit values for vulnerability difference map}
abs(max(fig5comp[[3]],na.rm = TRUE))
abs(min(fig5comp[[3]],na.rm = TRUE))
range <- max(abs(max(fig5comp[[3]],na.rm = TRUE)),abs(min(fig5comp[[3]],na.rm = TRUE)))
# takes the greatest value and makes it the range
``` 

```{r vulnerability difference map}
clrs = mutate(
  ea,
  colors = case_when(
    EA == "Missing Data" ~ "#999999",
    EA == "National Parks and Reserves" ~ "#D9EABB",
    EA == "Major Lakes of Malawi" ~ "lightblue"
  
  )
)$colors
vuln_diff_map = ggplot() +
  geom_sf(data = ea,
          fill = clrs,
          color = NA) +
  geom_stars(data = fig5comp["diff"]) +
  scale_fill_gradient2(
    low = "#e9a3c9",
    high = "#beaed4",
    breaks = c(-range,range),
    labels = c("Negative Difference", "Positive Difference"),
    na.value = "transparent",
    guide = "colourbar",
    limits = c(-range,range) #you want centered around zero, so make larger # the ends
  ) +
  scale_x_continuous(breaks = c(33,34,35,36)) +
  labs(title = "Fig. 5 Replication Comparison") +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
vuln_diff_map
```

```{r saving maps}

ggsave(
  here("results","figures","fig4comp_final.png"),
  plot = ac_diff_map,
  width = 8.5,
  height = 11,
  units = "in"
)
ggsave(
  here("results","figures","fig5comp_final.png"),
  plot = vuln_diff_map,
  width = 8.5,
  height = 11,
  units = "in"
)
```

```{r exporting comparison as geotiff}
write_stars(fig5comp, here("data", "derived", "public", "fig5diff.tif"), layer = "diff")
write_stars(fig5comp, here("data", "derived", "public", "fig5or.tif"), layer = "or")
write_stars(fig5comp, here("data", "derived", "public", "fig5rp.tif"), layer = "rp")
```